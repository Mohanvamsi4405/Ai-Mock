<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conversational AI Interview</title>
    <style>
        :root {
            --primary: #2563eb;
            --success: #059669;
            --danger: #dc2626;
            --warning: #d97706;
            --background: #f8fafc;
            --surface: #ffffff;
            --text: #1e293b;
            --text-light: #64748b;
            --border: #e2e8f0;
            --radius: 12px;
            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--background);
            color: var(--text);
            line-height: 1.6;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px; /* Base padding for desktop */
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            border-radius: var(--radius);
            text-align: center;
            margin-bottom: 30px;
        }

        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            font-weight: 700;
        }

        .conversation-flow {
            display: flex;
            justify-content: space-around;
            margin-top: 20px;
        }

        .flow-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-size: 0.9rem;
            opacity: 0.9;
            flex-basis: 30%; /* Give steps a defined width */
        }

        .flow-icon {
            font-size: 2rem;
            margin-bottom: 8px;
        }

        .progress-section {
            background: var(--surface);
            padding: 25px;
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            margin-bottom: 30px;
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background: var(--border);
            border-radius: 4px;
            margin-bottom: 20px;
            overflow: hidden;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--primary), var(--success));
            transition: width 0.5s ease;
            width: 25%;
        }

        .steps {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
        }

        .step {
            text-align: center;
            padding: 15px;
            background: var(--border);
            color: var(--text-light);
            border-radius: var(--radius);
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .step.active {
            background: var(--primary);
            color: white;
            transform: scale(1.05);
        }

        .step.completed {
            background: var(--success);
            color: white;
        }

        .content-section {
            display: none;
            background: var(--surface);
            padding: 30px;
            border-radius: var(--radius);
            box-shadow: var(--shadow);
            margin-bottom: 30px;
        }

        .content-section.active {
            display: block;
            animation: slideIn 0.5s ease;
        }

        @keyframes slideIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .upload-area {
            border: 3px dashed var(--border);
            border-radius: var(--radius);
            padding: 60px 30px;
            text-align: center;
            transition: all 0.3s ease;
            cursor: pointer;
            background: linear-gradient(45deg, #f8fafc, #f1f5f9);
        }

        .upload-area:hover, .upload-area.dragover {
            border-color: var(--primary);
            background: linear-gradient(45deg, #eff6ff, #dbeafe);
            transform: scale(1.02);
        }

        .upload-icon {
            font-size: 4rem;
            margin-bottom: 20px;
            display: block;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            padding: 14px 28px;
            border: none;
            border-radius: var(--radius);
            cursor: pointer;
            font-size: 1rem;
            font-weight: 600;
            text-decoration: none;
            transition: all 0.3s ease;
            text-align: center; /* Ensures text is centered in button */
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);
        }

        .btn-primary { background: var(--primary); color: white; }
        .btn-success { background: var(--success); color: white; }
        .btn-warning { background: var(--warning); color: white; }
        .btn-danger { background: var(--danger); color: white; }
        .btn-secondary { background: var(--text-light); color: white; }

        .btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }

        .form-group {
            margin-bottom: 20px;
        }

        .form-label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
        }

        .form-control {
            width: 100%;
            padding: 12px 16px;
            border: 2px solid var(--border);
            border-radius: var(--radius);
            font-size: 1rem;
            transition: border-color 0.3s ease;
        }

        .form-control:focus {
            outline: none;
            border-color: var(--primary);
            box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.1);
        }

        .interview-container {
            background: var(--surface);
            border-radius: var(--radius);
            padding: 30px;
            box-shadow: var(--shadow);
        }

        .conversation-status {
            background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);
            border: 2px solid var(--success);
            border-radius: var(--radius);
            padding: 20px;
            text-align: center;
            margin-bottom: 30px;
        }

        .status-indicator {
            font-size: 1.4rem;
            font-weight: 600;
            margin-bottom: 10px;
            color: var(--success);
        }

        .status-description {
            color: var(--text-light);
            font-size: 0.95rem;
        }

        .voice-interface {
            background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
            border: 3px solid var(--border);
            border-radius: var(--radius);
            padding: 30px;
            text-align: center;
            margin: 20px 0;
        }

        .voice-status {
            font-size: 1.3rem;
            font-weight: 600;
            margin-bottom: 20px;
            color: var(--text);
            min-height: 30px;
        }

        .voice-indicator {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            margin: 0 auto 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2.5rem;
            transition: all 0.3s ease;
        }

        .voice-indicator.idle {
            background: linear-gradient(135deg, var(--text-light), #94a3b8);
            color: white;
        }

        .voice-indicator.ai_speaking {
            background: linear-gradient(135deg, var(--primary), #3b82f6);
            color: white;
            animation: pulse 2s infinite;
        }

        .voice-indicator.listening_for_answer {
            background: linear-gradient(135deg, var(--success), #10b981);
            color: white;
            animation: pulse 2s infinite;
            box-shadow: 0 0 20px rgba(5, 150, 105, 0.5);
        }

        .voice-indicator.processing_answer {
            background: linear-gradient(135deg, var(--warning), #f59e0b);
            color: white;
            animation: spin 2s linear infinite;
        }

        .voice-indicator.ai_responding {
            background: linear-gradient(135deg, var(--primary), #3b82f6);
            color: white;
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .conversation-area {
            background: white;
            border: 2px solid var(--border);
            border-radius: var(--radius);
            height: 450px;
            overflow-y: auto;
            padding: 20px;
            margin: 20px 0;
            scroll-behavior: smooth;
        }

        .message {
            margin-bottom: 15px;
            padding: 15px 20px;
            border-radius: 15px;
            max-width: 85%;
            animation: messageAppear 0.4s ease-out;
            position: relative;
        }

        @keyframes messageAppear {
            from { 
                opacity: 0; 
                transform: translateY(15px) scale(0.9); 
            }
            to { 
                opacity: 1; 
                transform: translateY(0) scale(1); 
            }
        }

        .message.ai {
            background: linear-gradient(135deg, #eff6ff, #dbeafe);
            border-left: 4px solid var(--primary);
            margin-right: auto;
        }

        .message.user {
            background: linear-gradient(135deg, #f0fdf4, #dcfce7);
            border-right: 4px solid var(--success);
            margin-left: auto;
        }

        .message.system {
            background: linear-gradient(135deg, #fef3c7, #fde68a);
            border: 2px solid var(--warning);
            margin: 0 auto;
            max-width: 70%;
            text-align: center;
            font-style: italic;
        }

        .message-header {
            font-weight: 600;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 0.9rem;
            opacity: 0.8;
        }

        .message-content {
            font-size: 1rem;
            line-height: 1.5;
        }

        .thinking-indicator {
            background: linear-gradient(135deg, #f3f4f6, #e5e7eb);
            border-left: 4px solid var(--text-light);
            margin-right: auto;
            animation: none; /* remove pulse on the block itself */
        }

        .thinking-dots {
            display: flex;
            gap: 4px;
            align-items: center;
        }

        .thinking-dot {
            width: 8px;
            height: 8px;
            background: var(--text-light);
            border-radius: 50%;
            animation: thinkingDot 1.4s infinite ease-in-out both;
        }

        .thinking-dot:nth-child(1) { animation-delay: -0.32s; }
        .thinking-dot:nth-child(2) { animation-delay: -0.16s; }

        @keyframes thinkingDot {
            0%, 80%, 100% { transform: scale(0); }
            40% { transform: scale(1); }
        }

        .record-button {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, var(--danger), #ef4444);
            color: white;
            font-size: 2rem;
            cursor: pointer;
            transition: all 0.3s ease;
            margin: 20px auto;
            display: block;
            position: relative;
        }

        .record-button:hover {
            transform: scale(1.1);
            box-shadow: 0 10px 20px rgba(220, 38, 38, 0.4);
        }

        .record-button:active, .record-button.recording {
            background: linear-gradient(135deg, var(--success), #10b981);
            animation: pulse 1s infinite;
        }

        .record-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            animation: none; /* Stop pulse when disabled */
        }

        .record-instructions {
            text-align: center;
            color: var(--text-light);
            margin: 10px 0;
            font-size: 0.9rem;
        }

        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .stat {
            text-align: center;
            padding: 20px;
            background: var(--surface);
            border-radius: var(--radius);
            border: 2px solid var(--border);
        }

        .stat-value {
            font-size: 2rem;
            font-weight: 700;
            color: var(--primary);
            margin-bottom: 5px;
        }

        .stat-label {
            font-size: 0.9rem;
            color: var(--text-light);
            text-transform: uppercase;
            font-weight: 600;
        }

        .controls {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin: 30px 0;
            flex-wrap: wrap;
        }
        
        /* Make buttons in controls fill width on mobile */
        .controls .btn {
            flex: 1 1 auto;
            max-width: 300px; /* Max width on desktop */
        }

        .alert {
            padding: 15px 20px;
            border-radius: var(--radius);
            margin: 20px 0;
            font-weight: 500;
        }

        .alert.success {
            background: #d1fae5;
            border: 2px solid var(--success);
            color: #047857;
        }

        .alert.error {
            background: #fee2e2;
            border: 2px solid var(--danger);
            color: #991b1b;
        }

        .alert.warning {
            background: #fef3c7;
            border: 2px solid var(--warning);
            color: #92400e;
        }

        .loading {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            padding: 20px;
            font-weight: 600;
            color: var(--primary);
        }

        .spinner {
            width: 20px;
            height: 20px;
            border: 3px solid rgba(37, 99, 235, 0.3);
            border-top: 3px solid var(--primary);
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        .hidden { display: none !important; }

        /* --- MOBILE OPTIMIZATIONS (Max Width 768px) --- */
        @media (max-width: 768px) {
            .container { padding: 15px; }
            .header { padding: 30px 15px; }
            .header h1 { font-size: 1.8rem; }
            .header p { font-size: 0.9rem; }

            /* Improved conversation flow layout for mobile */
            .conversation-flow { 
                flex-direction: row; 
                flex-wrap: wrap; 
                gap: 10px;
            }
            .flow-step {
                flex-basis: 30%; /* Keep them spaced out horizontally */
                font-size: 0.8rem;
            }
            .flow-icon { font-size: 1.5rem; margin-bottom: 5px; }

            .steps { 
                /* Allow for two columns on smaller screens, or stack if necessary */
                grid-template-columns: repeat(2, 1fr); 
                gap: 10px;
            }
            .step { padding: 12px; font-size: 0.9rem; }

            .content-section { padding: 20px; }
            .upload-area { padding: 40px 20px; }
            .upload-icon { font-size: 3rem; margin-bottom: 15px; }
            .upload-area h3 { font-size: 1.1rem; }
            .upload-area p { font-size: 0.85rem; }

            .stats {
                /* Make stats stack vertically on very small screens */
                grid-template-columns: 1fr; 
                gap: 15px;
            }
            .stat-value { font-size: 1.8rem; }
            .stat-label { font-size: 0.8rem; }

            .voice-indicator { width: 70px; height: 70px; font-size: 1.8rem; }
            .voice-status { font-size: 1.1rem; }

            .conversation-area { height: 350px; padding: 15px; }
            .message { max-width: 95%; padding: 12px 16px; }

            .record-button { width: 70px; height: 70px; font-size: 1.8rem; }

            /* Stack all buttons in controls vertically on mobile */
            .controls { 
                flex-direction: column; 
                align-items: stretch; 
                gap: 10px;
            }
            .controls .btn {
                width: 100%;
                max-width: 100%; /* Override max-width for full mobile width */
                padding: 16px 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üé§ Conversational AI Interview</h1>
            <p>AI asks questions AND listens to your answers - like a real interview!</p>
            <div class="conversation-flow">
                <div class="flow-step">
                    <div class="flow-icon">ü§ñ</div>
                    <div>AI Asks Question</div>
                </div>
                <div class="flow-step">
                    <div class="flow-icon">üëÇ</div>
                    <div>AI Listens to You</div>
                </div>
                <div class="flow-step">
                    <div class="flow-icon">üí¨</div>
                    <div>AI Responds & Continues</div>
                </div>
            </div>
        </div>

        <div class="progress-section">
            <div class="progress-bar">
                <div class="progress-fill" id="progressFill"></div>
            </div>
            <div class="steps">
                <div class="step active" id="step1">Resume Upload</div>
                <div class="step" id="step2">Interview Setup</div>
                <div class="step" id="step3">AI Conversation</div>
                <div class="step" id="step4">Results</div>
            </div>
        </div>

        <div class="content-section active" id="section1">
            <h2>üìÑ Upload Your Resume</h2>
            <p>Upload your PDF resume for personalized interview questions.</p>

            <div class="upload-area" id="uploadArea">
                <span class="upload-icon">üìÅ</span>
                <h3>Drop PDF here or click to browse</h3>
                <p>Maximum 10MB &bullet; PDF format only</p>
                <input type="file" id="resumeFile" accept=".pdf" style="display: none;">
            </div>

            <div id="uploadResult"></div>
        </div>

        <div class="content-section" id="section2">
            <h2>üíº Interview Setup</h2>
            <p>Provide job details for targeted questions (optional).</p>

            <div class="form-group">
                <label class="form-label" for="jobRole">Job Role</label>
                <input type="text" class="form-control" id="jobRole" placeholder="e.g., Senior Developer, Data Scientist">
            </div>

            <div class="form-group">
                <label class="form-label" for="jobDescription">Job Description</label>
                <textarea class="form-control" id="jobDescription" rows="4" 
                    placeholder="Paste job description for more targeted questions..."></textarea>
            </div>

            <div class="controls">
                <button class="btn btn-primary" id="setupInterview">üöÄ Start Conversation</button>
                <button class="btn btn-secondary" id="skipJobDesc">Skip & Continue</button>
            </div>
        </div>

        <div class="content-section" id="section3">
            <h2>üí¨ AI Interview Conversation</h2>

            <div class="conversation-status" id="conversationStatus">
                <div class="status-indicator" id="statusIndicator">üîó Connecting to AI interviewer...</div>
                <div class="status-description" id="statusDescription">Preparing for your conversation</div>
            </div>

            <div class="stats">
                <div class="stat">
                    <div class="stat-value" id="currentQuestion">1</div>
                    <div class="stat-label">Current Question</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="totalQuestions">4</div>
                    <div class="stat-label">Total Questions</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="timer">00:00</div>
                    <div class="stat-label">Duration</div>
                </div>
            </div>

            <div class="interview-container">
                <div class="voice-interface">
                    <div class="voice-status" id="voiceStatus">Preparing conversation...</div>
                    <div class="voice-indicator idle" id="voiceIndicator">üé§</div>
                    <div id="voiceInstructions">The AI will speak to you and then listen for your response</div>
                </div>

                <div class="conversation-area" id="conversationArea">
                    <div class="message system">
                        <div class="message-content">Connecting to your AI interviewer...</div>
                    </div>
                </div>

                <button class="record-button" id="recordButton" disabled>üé§</button>
                <div class="record-instructions" id="recordInstructions">
                    Hold to record your response when AI is listening
                </div>
            </div>

            <div class="controls">
                <button class="btn btn-warning" id="nextQuestion">‚è≠Ô∏è Next Question</button>
                <button class="btn btn-danger" id="endInterview">üõë End Interview</button>
            </div>
        </div>

        <div class="content-section" id="section4">
            <h2>üìä Interview Complete!</h2>
            <p>Your conversational interview has ended successfully.</p>

            <div class="stats">
                <div class="stat">
                    <div class="stat-value" id="finalQuestions">0</div>
                    <div class="stat-label">Questions Answered</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="finalDuration">00:00</div>
                    <div class="stat-label">Total Duration</div>
                </div>
                <div class="stat">
                    <div class="stat-value" id="responseCount">0</div>
                    <div class="stat-label">Responses Given</div>
                </div>
            </div>

            <div class="conversation-area" id="finalTranscript">
                </div>

            <div class="controls">
                <button class="btn btn-primary" id="downloadTranscript">üíæ Download Transcript</button>
                <button class="btn btn-success" id="newInterview">üîÑ New Interview</button>
            </div>
        </div>
    </div>

    <script>
        // Global state
        let currentStep = 1;
        let sessionId = null;
        let websocket = null;
        let mediaRecorder = null;
        let audioStream = null;
        let isRecording = false;
        let conversationState = 'idle'; // idle, ai_speaking, listening_for_answer, processing_answer
        let startTime = null;
        let currentQuestionNum = 1;
        let totalQuestionsNum = 4;
        let speechSynthesis = window.speechSynthesis;
        let currentUtterance = null;

        // DOM elements
        const elements = {
            uploadArea: document.getElementById('uploadArea'),
            resumeFile: document.getElementById('resumeFile'),
            uploadResult: document.getElementById('uploadResult'),
            setupInterview: document.getElementById('setupInterview'),
            skipJobDesc: document.getElementById('skipJobDesc'),
            conversationStatus: document.getElementById('conversationStatus'),
            statusIndicator: document.getElementById('statusIndicator'),
            statusDescription: document.getElementById('statusDescription'),
            voiceStatus: document.getElementById('voiceStatus'),
            voiceIndicator: document.getElementById('voiceIndicator'),
            voiceInstructions: document.getElementById('voiceInstructions'),
            conversationArea: document.getElementById('conversationArea'),
            recordButton: document.getElementById('recordButton'),
            recordInstructions: document.getElementById('recordInstructions'),
            nextQuestion: document.getElementById('nextQuestion'),
            endInterview: document.getElementById('endInterview'),
            currentQuestion: document.getElementById('currentQuestion'),
            totalQuestions: document.getElementById('totalQuestions'),
            timer: document.getElementById('timer')
        };

        // Initialize app
        function init() {
            setupEventListeners();
            setupDragAndDrop();
            updateProgress();
            console.log('üé§ Conversational AI interview system initialized');
        }

        function setupEventListeners() {
            elements.uploadArea.addEventListener('click', () => elements.resumeFile.click());
            elements.resumeFile.addEventListener('change', handleFileUpload);
            elements.setupInterview.addEventListener('click', handleSetupInterview);
            elements.skipJobDesc.addEventListener('click', handleSkipJobDescription);

            // Record button - hold to record
            elements.recordButton.addEventListener('mousedown', startRecording);
            elements.recordButton.addEventListener('mouseup', stopRecording);
            elements.recordButton.addEventListener('mouseleave', stopRecording);
            elements.recordButton.addEventListener('touchstart', (e) => {
                e.preventDefault();
                startRecording();
            }, { passive: false });
            elements.recordButton.addEventListener('touchend', (e) => {
                e.preventDefault();
                stopRecording();
            }, { passive: false });

            elements.nextQuestion.addEventListener('click', handleNextQuestion);
            elements.endInterview.addEventListener('click', handleEndInterview);
            document.getElementById('downloadTranscript')?.addEventListener('click', downloadTranscript);
            document.getElementById('newInterview')?.addEventListener('click', () => location.reload());
            
            // Wait for voices to load for better speech synthesis quality
            if (speechSynthesis.onvoiceschanged !== undefined) {
                speechSynthesis.onvoiceschanged = () => {
                    console.log("Speech voices loaded.");
                };
            }
        }

        function setupDragAndDrop() {
            elements.uploadArea.addEventListener('dragover', (e) => {
                e.preventDefault();
                elements.uploadArea.classList.add('dragover');
            });

            elements.uploadArea.addEventListener('dragleave', () => {
                elements.uploadArea.classList.remove('dragover');
            });

            elements.uploadArea.addEventListener('drop', (e) => {
                e.preventDefault();
                elements.uploadArea.classList.remove('dragover');
                const files = e.dataTransfer.files;
                if (files.length > 0) {
                    handleFileSelection(files[0]);
                }
            });
        }

        function updateProgress() {
            const progressFill = document.getElementById('progressFill');
            // Calculate percentage based on current step / total steps
            const progressPercent = (currentStep / 4) * 100; 
            progressFill.style.width = progressPercent + '%';

            for (let i = 1; i <= 4; i++) {
                const step = document.getElementById('step' + i);
                const section = document.getElementById('section' + i);

                if (i < currentStep) {
                    step.classList.add('completed');
                    step.classList.remove('active');
                    if (section) section.classList.remove('active');
                } else if (i === currentStep) {
                    step.classList.add('active');
                    step.classList.remove('completed');
                    if (section) section.classList.add('active');
                } else {
                    step.classList.remove('active', 'completed');
                    if (section) section.classList.remove('active');
                }
            }
        }

        function showAlert(message, type = 'success') {
            const alertDiv = document.createElement('div');
            alertDiv.className = `alert ${type}`;
            alertDiv.innerHTML = message;
            elements.uploadResult.innerHTML = '';
            elements.uploadResult.appendChild(alertDiv);
        }

        async function handleFileUpload(event) {
            const file = event.target.files[0];
            if (file) await handleFileSelection(file);
        }

        async function handleFileSelection(file) {
            if (!file.type.includes('pdf')) {
                showAlert('Please select a PDF file.', 'error');
                return;
            }

            if (file.size > 10 * 1024 * 1024) {
                showAlert('File size must be less than 10MB.', 'error');
                return;
            }

            showAlert('<div class="loading"><div class="spinner"></div>Processing resume...</div>', 'warning');

            const formData = new FormData();
            formData.append('file', file);

            // Mocking API call for frontend-only code
            try {
                // In a real app, this would be your API endpoint
                // const response = await fetch('/upload-resume', { method: 'POST', body: formData });
                // const result = await response.json();
                
                // Mock result after a delay
                await new Promise(resolve => setTimeout(resolve, 1500));
                const mockResult = {
                    success: true,
                    sessionId: 'mock-session-12345',
                    parsedContent: {
                        skills: ['JavaScript', 'React', 'Python', 'AWS', 'Docker', 'SQL', 'Git', 'Agile'],
                        experience: [1, 2, 3]
                    }
                };
                const result = mockResult;

                if (result.success) {
                    sessionId = result.sessionId;
                    showAlert(`‚úÖ <strong>Resume processed successfully!</strong><br>
                        üìä Key Skills: ${result.parsedContent.skills.slice(0, 5).join(', ')}...<br>
                        üìù Experience entries: ${result.parsedContent.experience.length}`, 'success');

                    setTimeout(() => {
                        currentStep = 2;
                        updateProgress();
                    }, 2000);
                } else {
                    showAlert(result.message || 'Failed to process resume', 'error');
                }
            } catch (error) {
                showAlert('Upload failed: ' + error.message, 'error');
            }
        }

        async function handleSetupInterview() {
            if (!sessionId) {
                // If skipping upload in a real scenario, you'd generate a session now.
                sessionId = 'mock-session-general'; 
            }

            const jobRole = document.getElementById('jobRole').value.trim();
            const jobDescription = document.getElementById('jobDescription').value.trim();

            const requestData = { session_id: sessionId };
            if (jobRole || jobDescription) {
                requestData.job_description = {
                    role: jobRole || 'General Position',
                    text: jobDescription || 'General interview questions'
                };
            }

            // Mocking API call for interview setup
            try {
                // In a real app: fetch('/setup-interview', { method: 'POST', body: JSON.stringify(requestData) });
                
                // Mock result
                await new Promise(resolve => setTimeout(resolve, 1500));
                const mockResult = {
                    success: true,
                    questions: Array(4).fill(0), // 4 questions
                    message: 'Setup successful'
                };
                const result = mockResult;

                if (result.success) {
                    totalQuestionsNum = result.questions.length;
                    elements.totalQuestions.textContent = totalQuestionsNum;

                    // Clear previous alert messages from step 1
                    document.getElementById('uploadResult').innerHTML = ''; 

                    setTimeout(async () => {
                        currentStep = 3;
                        updateProgress();
                        // This starts the conversation
                        await startConversationalInterview(); 
                    }, 100); 
                } else {
                    showAlert(result.message || 'Failed to setup interview', 'error');
                }
            } catch (error) {
                showAlert('Setup failed: ' + error.message, 'error');
            }
        }

        function handleSkipJobDescription() {
            handleSetupInterview();
        }

        async function startConversationalInterview() {
            if (!sessionId) return;

            updateConversationStatus('üîó Requesting microphone access...', 'Please allow microphone access for the conversation');
            elements.recordButton.disabled = true; // Disable button until fully ready

            try {
                // Get microphone access
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 44100
                    }
                });

                // Mock WebSocket setup and message flow
                setupMockWebSocket();
                startTimer();
                updateConversationStatus('ü§ñ Connected to AI interviewer', 'Preparing for conversation');

            } catch (error) {
                console.error('Microphone access denied:', error);
                showAlert('Microphone access is required for voice conversation. Please allow access and refresh.', 'error');
                updateConversationStatus('‚ùå Microphone access denied', 'Please refresh and allow microphone access');
            }
        }

        // --- MOCK WEBSOCKET & CONVERSATION LOGIC ---
        let mockMessageQueue = [
            { type: 'ai_message', content: 'Hello, I will be your interviewer today. We will go through a few questions about your background and qualifications. To answer, hold the mic button, speak, and release. Ready to start?', speak: true },
            { type: 'start_listening' },
            { type: 'user_response_shown', content: 'I am ready. I have prepared for this interview by studying the company and the job description, and I\'m excited to discuss my experience.' },
            { type: 'processing_response' },
            { type: 'ai_conversational_response', content: "That's great to hear! Let's begin with the first question. Tell me about a complex project you led. How did you organize the team, manage unexpected roadblocks, and ensure its successful delivery?", speak: true, questionNumber: 1 },
            { type: 'start_listening' },
            { type: 'user_response_shown', content: 'My last major project involved migrating our core database to a cloud-based solution. The key challenge was minimizing downtime. I structured the team into three workstreams: planning, migration, and testing. We hit a snag when a legacy system showed unexpected schema incompatibilities, which we fixed by building a small microservice layer as an intermediary.' },
            { type: 'processing_response' },
            { type: 'ai_conversational_response', content: 'That sounds like a significant challenge. Building an intermediary service to handle schema incompatibility shows excellent technical problem-solving. Could you elaborate on how you communicated the *risk* of this roadblock to your non-technical stakeholders?', speak: true },
            { type: 'start_listening' },
            { type: 'user_response_shown', content: 'I communicated the risk by framing it in terms of business impact, specifically a potential 48-hour delay, instead of technical jargon. I presented two options: the microservice fix with the delay, or a higher-risk, rushed migration. They opted for the safer, delayed path.' },
            { type: 'processing_response' },
            { type: 'ai_question', content: "Great. Moving to question number two: Your resume highlights strong experience in JavaScript and Python. Which of these languages do you believe is better suited for building scalable, high-performance back-end systems, and why?", speak: true, questionNumber: 2 },
            { type: 'start_listening' },
            { type: 'user_response_shown', content: 'For scalable, high-performance back-end systems, I lean toward Python, specifically using frameworks like FastAPI or Django, mainly because of the vast ecosystem of scientific and data libraries. However, Node.js (JavaScript) offers superior I/O performance due to its non-blocking event loop, which is critical for I/O bound tasks like microservices.' },
            { type: 'processing_response' },
            { type: 'ai_conversational_response', content: 'That‚Äôs a nuanced answer. It sounds like you value Python‚Äôs ecosystem but recognize Node.js‚Äôs I/O strengths. Can you give me a recent example where you had to prioritize one over the other for a production system?', speak: true },
            { type: 'start_listening' },
            { type: 'user_response_shown', content: 'We built a data processing pipeline that was heavily CPU bound, involving complex matrix calculations. We chose Python with NumPy for that part. For the API gateway, which was purely I/O bound, we used Node.js. It was a polyglot system to leverage the strengths of both.' },
            { type: 'processing_response' },
            { type: 'ai_question', content: "Excellent strategy. Final question for today (Question 3 of 3): Describe a time you received constructive criticism that drastically changed how you approached your work. What was the criticism, and what did you change?", speak: true, questionNumber: 3 },
            { type: 'start_listening' },
            { type: 'user_response_shown', content: 'The criticism was about my tendency to over-engineer solutions, making them technically beautiful but difficult for my teammates to maintain. I was told, "Perfect is the enemy of done." I changed my approach by introducing a mandatory design review stage focused solely on simplicity and maintainability, ensuring I built for the team, not just for myself.' },
            { type: 'processing_response' },
            { type: 'ai_conversational_response', content: 'That‚Äôs a very insightful takeaway. Thank you for sharing your thoughtful answers. I have enough information to proceed with the evaluation.', speak: true },
            { type: 'interview_completed', content: 'Thank you for participating in this conversational interview. Please click "End Interview" to view your final transcript and details.' }
        ];
        
        // This function simulates the behavior of a real-time WebSocket connection
        function setupMockWebSocket() {
            let mockIndex = 0;

            // Simulate immediate message flow for the introduction
            setTimeout(() => {
                // Send first message immediately
                handleWebSocketMessage({ data: JSON.stringify(mockMessageQueue[mockIndex++]) });
            }, 500);

            // Mock the next question button click/AI state transition
            elements.nextQuestion.onclick = () => {
                if (currentUtterance) speechSynthesis.cancel(); // Stop AI speech if interrupted
                // Check if the current listening state is waiting for the next user response
                while (mockIndex < mockMessageQueue.length && mockMessageQueue[mockIndex].type !== 'ai_question' && mockMessageQueue[mockIndex].type !== 'interview_completed') {
                    mockIndex++;
                }
                
                if (mockIndex < mockMessageQueue.length) {
                    handleWebSocketMessage({ data: JSON.stringify(mockMessageQueue[mockIndex++]) });
                }
            };
            
            // This function is the core mock for sending the response
            async function sendMockResponse() {
                if (currentUtterance) speechSynthesis.cancel(); // Stop any current AI speech
                
                // Simulate the immediate transition to processing
                handleWebSocketMessage({ data: JSON.stringify({ type: 'processing_response' }) });

                // Find the next sequence of messages: user response -> processing -> ai response/question
                // We'll advance the index three times to skip the user message and processing step
                const userResponse = mockMessageQueue[mockIndex++];
                const processing = mockMessageQueue[mockIndex++];
                const aiResponse = mockMessageQueue[mockIndex++];

                // Wait for a mock processing time (1.5 seconds)
                await new Promise(resolve => setTimeout(resolve, 1500)); 

                // Send the mock user response back to the client
                handleWebSocketMessage({ data: JSON.stringify(userResponse) });

                // Send the mock AI response/question to the client
                await new Promise(resolve => setTimeout(resolve, 500));
                handleWebSocketMessage({ data: JSON.stringify(aiResponse) });

                // If the next logical state is listening, send it
                if (mockIndex < mockMessageQueue.length && (mockMessageQueue[mockIndex].type === 'start_listening' || mockMessageQueue[mockIndex].type === 'continue_listening')) {
                    handleWebSocketMessage({ data: JSON.stringify(mockMessageQueue[mockIndex++]) });
                }
            }
            
            // Re-map the actual audio sending function to the mock function
            window.sendAudioResponse = sendMockResponse;
        }
        
        // Use the mock function in the stopRecording to trigger the conversation flow
        async function sendAudioResponse(audioBlob) {
            // Placeholder for the original function; now pointing to the mock
            window.sendAudioResponse(); 
        }
        // --- END MOCK WEBSOCKET & CONVERSATION LOGIC ---

        function handleWebSocketMessage(event) {
            // Note: The mock flow provides the `data` object directly
            const data = JSON.parse(event.data);
            console.log('üì® Received:', data.type, 'State:', data.state);

            switch (data.type) {
                case 'ai_message':
                case 'ai_question':
                    conversationState = 'ai_speaking';
                    addMessage('ai', data.content);
                    updateVoiceInterface('üó£Ô∏è AI is speaking...', 'ai_speaking', 'The AI interviewer is talking');

                    if (data.speak) {
                        // Stop any previous speech before starting a new one
                        if (currentUtterance) speechSynthesis.cancel();
                        speakText(data.content, () => {
                            // After AI finishes speaking, check the mock queue for the next state (usually 'start_listening')
                            if (mockMessageQueue[mockIndex] && mockMessageQueue[mockIndex].type.endsWith('listening')) {
                                handleWebSocketMessage({ data: JSON.stringify(mockMessageQueue[mockIndex++]) });
                            }
                        });
                    }

                    if (data.questionNumber) {
                        currentQuestionNum = data.questionNumber;
                        elements.currentQuestion.textContent = currentQuestionNum;
                    }
                    break;

                case 'start_listening':
                case 'continue_listening':
                case 'resume_listening':
                    conversationState = 'listening_for_answer';
                    updateConversationStatus('üëÇ AI is listening for your response', 'Hold the record button and speak your answer');
                    updateVoiceInterface('üëÇ Listening for your response...', 'listening_for_answer', 'Hold the record button to speak');
                    elements.recordButton.disabled = false;
                    elements.recordInstructions.textContent = 'Hold the button and speak your answer';
                    removeThinkingIndicator();
                    break;

                case 'processing_response':
                    conversationState = 'processing_answer';
                    updateVoiceInterface('‚öôÔ∏è Processing your response...', 'processing_answer', 'AI is thinking about your answer');
                    elements.recordButton.disabled = true;
                    showThinkingIndicator();
                    break;

                case 'user_response_shown':
                    addMessage('user', data.content);
                    removeThinkingIndicator();
                    console.log('üë§ User said:', data.content);
                    break;

                case 'ai_conversational_response':
                    conversationState = 'ai_speaking';
                    addMessage('ai', data.content);
                    updateVoiceInterface('üó£Ô∏è AI is responding...', 'ai_responding', 'AI is having a conversation with you');

                    if (data.speak) {
                        if (currentUtterance) speechSynthesis.cancel();
                        speakText(data.content, () => {
                            // After conversational response, move to next state (usually continue_listening or ask_question)
                            if (mockMessageQueue[mockIndex]) {
                                handleWebSocketMessage({ data: JSON.stringify(mockMessageQueue[mockIndex++]) });
                            }
                        });
                    }
                    break;

                case 'interview_completed':
                    conversationState = 'completed';
                    addMessage('ai', data.content);
                    updateConversationStatus('üéâ Interview completed!', 'The AI has finished asking questions');
                    updateVoiceInterface('‚úÖ Interview finished', 'idle', 'Conversation complete');

                    if (data.speak) {
                        if (currentUtterance) speechSynthesis.cancel();
                        speakText(data.content);
                    }

                    elements.recordButton.disabled = true;
                    removeThinkingIndicator();

                    setTimeout(() => {
                        currentStep = 4;
                        updateProgress();
                        showResults();
                    }, 4000);
                    break;

                case 'error':
                    console.error('Server error:', data.message);
                    addMessage('system', '‚ùå Error: ' + data.message);
                    updateConversationStatus('‚ùå Error occurred', data.message);
                    break;
            }
        }

        function addMessage(type, content) {
            // Remove thinking indicator if present
            removeThinkingIndicator();

            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${type}`;

            let icon = 'ü§ñ';
            let sender = 'AI Interviewer';

            if (type === 'user') {
                icon = 'üë§';
                sender = 'You';
            } else if (type === 'system') {
                icon = 'üîß';
                sender = 'System';
            }

            if (type !== 'system') {
                messageDiv.innerHTML = `
                    <div class="message-header">${icon} ${sender}</div>
                    <div class="message-content">${content}</div>
                `;
            } else {
                messageDiv.innerHTML = `
                    <div class="message-content">${content}</div>
                `;
            }

            elements.conversationArea.appendChild(messageDiv);
            elements.conversationArea.scrollTop = elements.conversationArea.scrollHeight;
        }

        function showThinkingIndicator() {
            removeThinkingIndicator();

            const thinkingDiv = document.createElement('div');
            thinkingDiv.className = 'message thinking-indicator';
            thinkingDiv.id = 'thinkingIndicator';
            thinkingDiv.innerHTML = `
                <div class="message-header">ü§ñ AI Interviewer</div>
                <div class="message-content">
                    <div class="thinking-dots">
                        <div class="thinking-dot"></div>
                        <div class="thinking-dot"></div>
                        <div class="thinking-dot"></div>
                        <span style="margin-left: 10px;">thinking about your response...</span>
                    </div>
                </div>
            `;

            elements.conversationArea.appendChild(thinkingDiv);
            elements.conversationArea.scrollTop = elements.conversationArea.scrollHeight;
        }

        function removeThinkingIndicator() {
            const existing = document.getElementById('thinkingIndicator');
            if (existing) {
                existing.remove();
            }
        }

        function speakText(text, onendCallback) {
            if (!text) {
                if(onendCallback) onendCallback();
                return;
            }

            // Stop any current speech
            speechSynthesis.cancel();

            currentUtterance = new SpeechSynthesisUtterance(text);
            currentUtterance.rate = 0.9;
            currentUtterance.volume = 0.8;
            currentUtterance.pitch = 1.0;

            // Try to use a better voice
            const voices = speechSynthesis.getVoices();
            const preferredVoice = voices.find(voice => 
                voice.name.includes('Google') || 
                voice.name.includes('Microsoft') ||
                (voice.lang.includes('en') && voice.name.includes('Female'))
            );
            if (preferredVoice) {
                currentUtterance.voice = preferredVoice;
            }

            currentUtterance.onend = () => {
                console.log('üîä AI finished speaking');
                if (onendCallback) onendCallback();
            };

            currentUtterance.onerror = (error) => {
                console.error('Speech error:', error);
                if (onendCallback) onendCallback();
            };

            speechSynthesis.speak(currentUtterance);
        }

        async function startRecording() {
            if (isRecording || !audioStream || conversationState !== 'listening_for_answer') {
                return;
            }
            
            // Cancel any AI speech synthesis if it's still talking (user interrupts)
            if (speechSynthesis.speaking) {
                speechSynthesis.cancel();
            }

            try {
                mediaRecorder = new MediaRecorder(audioStream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                let audioChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    if (audioChunks.length > 0) {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        // Call the mock function instead of the real sendAudioResponse
                        await sendAudioResponse(audioBlob); 
                    }
                };

                mediaRecorder.start();
                isRecording = true;
                elements.recordButton.classList.add('recording');
                elements.recordInstructions.textContent = 'Recording... release to send';

            } catch (error) {
                console.error('Failed to start recording:', error);
            }
        }

        function stopRecording() {
            if (!isRecording) return;

            isRecording = false;
            elements.recordButton.classList.remove('recording');
            elements.recordInstructions.textContent = 'Processing your response...';
            elements.recordButton.disabled = true;

            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
        }
        
        // This is a placeholder/mock of the original function's purpose
        // The mock logic for sending the response is now inside setupMockWebSocket
        // The original code has been updated above to call the mock logic.
        /*
        async function sendAudioResponse(audioBlob) {
            // ... original websocket audio sending logic ...
        }
        */

        function updateConversationStatus(indicator, description) {
            elements.statusIndicator.textContent = indicator;
            elements.statusDescription.textContent = description;
        }

        function updateVoiceInterface(status, state, instructions) {
            elements.voiceStatus.textContent = status;
            elements.voiceIndicator.className = `voice-indicator ${state}`;
            elements.voiceInstructions.textContent = instructions;

            const icons = {
                idle: 'üé§',
                ai_speaking: 'üó£Ô∏è',
                listening_for_answer: 'üëÇ',
                processing_answer: '‚öôÔ∏è',
                ai_responding: 'üí¨'
            };

            elements.voiceIndicator.textContent = icons[state] || 'üé§';
        }

        function handleNextQuestion() {
            // In the mock, this advances the mock index until the next question or completion
            if (currentUtterance) speechSynthesis.cancel();
            
            // Only proceed if not already processing
            if (conversationState !== 'processing_answer') {
                // Mock the server call
                elements.recordButton.disabled = true;
                elements.recordInstructions.textContent = 'Moving to next question...';

                // Skip any intervening conversational responses
                while (mockIndex < mockMessageQueue.length && mockMessageQueue[mockIndex].type !== 'ai_question' && mockMessageQueue[mockIndex].type !== 'interview_completed') {
                    mockIndex++;
                }

                if (mockIndex < mockMessageQueue.length) {
                    handleWebSocketMessage({ data: JSON.stringify(mockMessageQueue[mockIndex++]) });
                } else {
                    // Force completion if 'Next Question' is pressed at the end
                    handleEndInterview(true);
                }
            }
        }

        function handleEndInterview(force = false) {
            if (force || confirm('End the interview? Your conversation will be saved.')) {
                // Stop any AI speech
                if (currentUtterance) speechSynthesis.cancel(); 
                
                // Set final stats and move to step 4
                currentStep = 4;
                updateProgress();
                showResults();

                // Close resources
                if (websocket) websocket.close();
                if (audioStream) {
                    audioStream.getTracks().forEach(track => track.stop());
                }
            }
        }

        function startTimer() {
            startTime = Date.now();
            // Use a variable to store the interval ID
            window.timerInterval = setInterval(() => {
                if (startTime) {
                    const elapsed = Date.now() - startTime;
                    const minutes = Math.floor(elapsed / 60000);
                    const seconds = Math.floor((elapsed % 60000) / 1000);
                    elements.timer.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
                }
            }, 1000);
        }

        function showResults() {
            // Stop the timer
            if (window.timerInterval) clearInterval(window.timerInterval);
            
            const finalDuration = elements.timer.textContent;
            const responseCount = document.querySelectorAll('.message.user').length;

            document.getElementById('finalQuestions').textContent = currentQuestionNum;
            document.getElementById('finalDuration').textContent = finalDuration;
            document.getElementById('responseCount').textContent = responseCount;

            const finalTranscript = document.getElementById('finalTranscript');
            // Clone the HTML content of the conversation area
            finalTranscript.innerHTML = elements.conversationArea.innerHTML; 
            
            // Remove the temporary thinking indicator if it was cloned
            const thinkingOnResults = finalTranscript.querySelector('.thinking-indicator');
            if (thinkingOnResults) thinkingOnResults.remove();
        }

        function downloadTranscript() {
            const messages = document.querySelectorAll('.message:not(.thinking-indicator)');
            let transcript = 'Conversational AI Interview Transcript\n';
            transcript += '======================================\n\n';
            transcript += `Date: ${new Date().toLocaleDateString()}\n`;
            transcript += `Duration: ${elements.timer.textContent}\n\n`;

            messages.forEach(message => {
                const header = message.querySelector('.message-header');
                const content = message.querySelector('.message-content').textContent.trim();

                if (header) {
                    transcript += `${header.textContent}: ${content}\n\n`;
                } else {
                    // System messages (like connecting/errors)
                    transcript += `System: ${content}\n\n`;
                }
            });

            const blob = new Blob([transcript], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `conversational_interview_${new Date().toISOString().split('T')[0]}.txt`;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }

        // Initialize when page loads
        document.addEventListener('DOMContentLoaded', init);

        // Clean up on page unload
        window.addEventListener('beforeunload', () => {
            if (websocket) websocket.close();
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
            }
            if (currentUtterance) {
                speechSynthesis.cancel();
            }
        });
    </script>
</body>
</html>
